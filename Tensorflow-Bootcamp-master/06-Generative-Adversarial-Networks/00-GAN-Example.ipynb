{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.examples.tutorials'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1b9e9c7674f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../03-Convolutional-Neural-Networks/MNIST_data/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples.tutorials'"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../03-Convolutional-Neural-Networks/MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb0afe713a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[5].reshape(28,28),cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Networks\n",
    "\n",
    "Useful Links:\n",
    "\n",
    "https://stackoverflow.com/questions/45307072/using-leaky-relu-in-tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z,reuse=None):\n",
    "    with tf.variable_scope('gen',reuse=reuse):\n",
    "        hidden1 = tf.layers.dense(inputs=z,units=128)\n",
    "        # Leaky Relu\n",
    "        alpha = 0.01\n",
    "        hidden1 = tf.maximum(alpha*hidden1,hidden1)\n",
    "        hidden2 = tf.layers.dense(inputs=hidden1,units=128)\n",
    "        \n",
    "        hidden2 = tf.maximum(alpha*hidden2,hidden2)\n",
    "        output = tf.layers.dense(hidden2,units=784,activation=tf.nn.tanh)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(X,reuse=None):\n",
    "    with tf.variable_scope('dis',reuse=reuse):\n",
    "        hidden1 = tf.layers.dense(inputs=X,units=128)\n",
    "        # Leaky Relu\n",
    "        alpha = 0.01\n",
    "        hidden1 = tf.maximum(alpha*hidden1,hidden1)\n",
    "        \n",
    "        hidden2 = tf.layers.dense(inputs=hidden1,units=128)\n",
    "        hidden2 = tf.maximum(alpha*hidden2,hidden2)\n",
    "        \n",
    "        logits = tf.layers.dense(hidden2,units=1)\n",
    "        output = tf.sigmoid(logits)\n",
    "    \n",
    "        return output, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images = tf.placeholder(tf.float32,shape=[None,784])\n",
    "z = tf.placeholder(tf.float32,shape=[None,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-a764f31adbd9>:3: dense (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "G = generator(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_output_real , D_logits_real = discriminator(real_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_output_fake, D_logits_fake = discriminator(G,reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(logits_in,labels_in):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_in,labels=labels_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_real_loss = loss_func(D_logits_real,tf.ones_like(D_logits_real)* (0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_fake_loss = loss_func(D_logits_fake,tf.zeros_like(D_logits_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_loss = D_real_loss + D_fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_loss = loss_func(D_logits_fake,tf.ones_like(D_logits_fake))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dis/dense/kernel:0', 'dis/dense/bias:0', 'dis/dense_1/kernel:0', 'dis/dense_1/bias:0', 'dis/dense_2/kernel:0', 'dis/dense_2/bias:0']\n",
      "['gen/dense/kernel:0', 'gen/dense/bias:0', 'gen/dense_1/kernel:0', 'gen/dense_1/bias:0', 'gen/dense_2/kernel:0', 'gen/dense_2/bias:0']\n"
     ]
    }
   ],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "\n",
    "d_vars = [var for var in tvars if 'dis' in var.name]\n",
    "g_vars = [var for var in tvars if 'gen' in var.name]\n",
    "\n",
    "print([v.name for v in d_vars])\n",
    "print([v.name for v in g_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_trainer = tf.train.AdamOptimizer(learning_rate).minimize(D_loss, var_list=d_vars)\n",
    "G_trainer = tf.train.AdamOptimizer(learning_rate).minimize(G_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 500\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(var_list=g_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a sample per epoch\n",
    "samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'num_examples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-77970c02a625>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# // indicates classic division\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_examples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'num_examples'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    # Recall an epoch is an entire run through the training data\n",
    "    for e in range(epochs):\n",
    "        # // indicates classic division\n",
    "        num_batches = train_images.num_examples // batch_size\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            \n",
    "            # Grab batch of images\n",
    "            batch = train_images.next_batch(batch_size)\n",
    "            \n",
    "            # Get images, reshape and rescale to pass to D\n",
    "            batch_images = batch[0].reshape((batch_size, 784))\n",
    "            batch_images = batch_images*2 - 1\n",
    "            \n",
    "            # Z (random latent noise data for Generator)\n",
    "            # -1 to 1 because of tanh activation\n",
    "            batch_z = np.random.uniform(-1, 1, size=(batch_size, 100))\n",
    "            \n",
    "            # Run optimizers, no need to save outputs, we won't use them\n",
    "            _ = sess.run(D_trainer, feed_dict={real_images: batch_images, z: batch_z})\n",
    "            _ = sess.run(G_trainer, feed_dict={z: batch_z})\n",
    "        \n",
    "            \n",
    "        print(\"Currently on Epoch {} of {} total...\".format(e+1, epochs))\n",
    "        \n",
    "        # Sample from generator as we're training for viewing afterwards\n",
    "        sample_z = np.random.uniform(-1, 1, size=(1, 100))\n",
    "        gen_sample = sess.run(generator(z ,reuse=True),feed_dict={z: sample_z})\n",
    "        \n",
    "        samples.append(gen_sample)\n",
    "        \n",
    "#         saver.save(sess, './models/500_epoch_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/500_epoch_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(var_list=g_vars)\n",
    "\n",
    "new_samples = []\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess,'./models/500_epoch_model.ckpt')\n",
    "    \n",
    "    for x in range(5):\n",
    "        sample_z = np.random.uniform(-1,1,size=(1,100))\n",
    "        gen_sample = sess.run(generator(z,reuse=True),feed_dict={z:sample_z})\n",
    "        \n",
    "        new_samples.append(gen_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ad97639a4716>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Greys'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "plt.imshow(samples[0].reshape(28,28),cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
